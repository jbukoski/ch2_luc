pts2000_df
unique(pts2000_df )
sort(unique(pts2000_df))
unique(pts2000_df) %>% arrange(code_num)
pts2014_df <- ptsData2014
st_geometry(pts2014_df) <- NULL
unique(pts2014_df) %>% arrange(code_num)
lulc2000
lulc2014
lulc2000 <- read_sf("~/Documents/dmcr_data/Land use (2000 and 2014)/MG_TYPE_43.shp") %>%
dplyr::select(code = CODE) %>%
mutate(code = ifelse(code %in% c("Mi"), "Unk", code))
pts2000 <- st_sample(lulc2000, 2500, type = "random", exact = TRUE)
ptsData2000 <- st_intersection(lulc2000, pts2000) %>%
st_intersection(thai) %>%
mutate(code_num = as.numeric(as.factor(code))) %>%
dplyr::select(code, code_num)
pts2000_df <- ptsData2000
st_geometry(pts2000_df) <- NULL
pts2000_df %>%
unique() %>%
arrange(code_num)
pts2014_df %>%
unique() %>%
arrange(code_num)
st_write(ptsData2000, "~/Documents/dmcr_data/", layer = "ptsData2000", driver = "ESRI Shapefile")
st_write(ptsData2000, "~/Documents/dmcr_data/", layer = "ptsData2000", driver = "ESRI Shapefile")
st_write(ptsData2014, "~/Documents/dmcr_data/", layer = "ptsData2014", driver = "ESRI Shapefile")
st_write(ptsData2014, "~/Documents/dmcr_data/", layer = "ptsData2014", driver = "ESRI Shapefile")
st_write(ptsData2014, "~/Documents/dmcr_data/", layer = "ptsData_2014", driver = "ESRI Shapefile")
st_write(ptsData2000, "~/Documents/dmcr_data/", layer = "ptsData_2000", driver = "ESRI Shapefile")
st_write(ptsData2014, "~/Documents/dmcr_data/", layer = "ptsData_2014", driver = "ESRI Shapefile")
st_write(ptsData2000, "~/Documents/dmcr_data/pts2000", layer = "ptsData_2000", driver = "ESRI Shapefile")
st_write(ptsData2014, "~/Documents/dmcr_data/pts2014", layer = "ptsData_2014", driver = "ESRI Shapefile")
library(randomForest)
newDat <- read_sf("~/Documents/dmcr_data/drive-download-20200226T232913Z-001/exported2014points.shp")
st_geometry(newDat) <- NULL
dat <- drop_na(newDat) %>%
mutate(code_num = as.factor(CODE),
idx = row_number()) %>%
filter(!(code_num %in% c(3, 4, 8))) %>%
dplyr::select(code_num, ndmi, mmri, mndwi, evi, diff, ndvi, ndwi, dtm, ndsi, idx)
dat <- drop_na(newDat) %>%
mutate(code_num = as.factor(code),
idx = row_number()) %>%
filter(!(code_num %in% c(3, 4, 8))) %>%
dplyr::select(code_num, ndmi, mmri, mndwi, evi, diff, ndvi, ndwi, dtm, ndsi, idx)
newDat <- read_sf("~/Documents/dmcr_data/drive-download-20200226T232913Z-001/exported2014points.shp")
newDat
newDat <- newDat %>%
rename(code_num = code,
first = predicted)
newDat <- newDat %>%
rename(code = code_num,
pred = first)
newDat
newDat <- newDat %>%
rename(orig = code_num,
pred = first)
newDat <- read_sf("~/Documents/dmcr_data/drive-download-20200226T232913Z-001/exported2014points.shp")
newDat <- newDat %>%
rename(orig = code_num,
pred = first)
newDat
cm <- table(newDat$orig, newDat$pred)
cm
newDat %>% group_by(code) %>% summarize(no = length(code))
newDat %>% group_by(pred) %>% summarize(no = length(code))
newDat %>% group_by(code) %>% summarize(no = length(code))
colsums(cm)
colsum(cm)
colSums(cm)
1186/1467
153/273
179/358
pts2000df %>% group_by(code) %>% summarize(no = length(code))
pts2000_df %>% group_by(code) %>% summarize(no = length(code))
lulc2000
lulc2000 <- read_sf("~/Documents/dmcr_data/Land use (2000 and 2014)/MG_TYPE_43.shp")
lulc2000
sum(lulc2000$AREA_RAI)
a <- c(5, 7)
b <- c(1, 9)
c <- c(3, 8)
d <- c(a, b, c)
mean(d)
mean(mean(a), mean(b), mean(c))
d
mean(d)
mean(a)
mean(b)
mean(c)
mean(6, 5, 5.5)
mean(c(6, 5, 5.5))
mean(c(mean(a), mean(b), mean(c)))
runif(9)
a <- 10* runif(9)
a
b <- 10 * runif(9)
mean(a)
mean(b)
mean(c(mean(a), mean(b)))
mean(c(a, b))
library(tidyverse)
dir <- "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/csv/"
files <- list.files(dir)
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = c("pearson"))
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
corr
write_csv(summary, "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_pearson.csv")
write_csv(corr, "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_pearson.csv")
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = c("spearman"))
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
?cor.test
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = c("spearman"))
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
corr
write_csv(summary, "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_pearson.csv")
write_csv(corr, "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_pearson.csv")
corrType = "spearman"
files <- list.files(dir)
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = corrType)
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
corr
write_csv(summary, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_", corrType, ".csv"))
write_csv(corr, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_", corrType, ".csv"))
corrType = "pearson"
files <- list.files(dir)
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = corrType)
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
corr
write_csv(summary, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_", corrType, ".csv"))
write_csv(corr, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_", corrType, ".csv"))
sum_dat
summary
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$p.val
var <- unique(c(var1,var2))
pval <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
pval[lower.tri(pval, diag = FALSE)] <- r # lower triangular matrix to be r
pval <- Matrix::forceSymmetric(pval, uplo="L")
pval <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
pval[lower.tri(pval, diag = FALSE)] <- r # lower triangular matrix to be r
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr, p.val)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$p.val
var <- unique(c(var1,var2))
pval <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
pval[lower.tri(pval, diag = FALSE)] <- r # lower triangular matrix to be r
pval <- Matrix::forceSymmetric(pval, uplo="L")
pval <- as.matrix(pval)
pval <- as.data.frame(pval) # formatting
row.names(pval) <- var # row names
colnames(pval) <- var # column names
pval
library(tidyverse)
dir <- "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/csv/"
corrType = "pearson"
files <- list.files(dir)
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = corrType)
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr, p.val)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$p.val
var <- unique(c(var1,var2))
pval <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
pval[lower.tri(pval, diag = FALSE)] <- r # lower triangular matrix to be r
pval <- Matrix::forceSymmetric(pval, uplo="L")
pval <- as.matrix(pval)
pval <- as.data.frame(pval) # formatting
row.names(pval) <- var # row names
colnames(pval) <- var # column names
pval
write_csv(summary, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_", corrType, ".csv"))
write_csv(corr, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_", corrType, ".csv"))
write_csv(pval, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/pvalMatrix_", corrType, ".csv"))
library(tidyverse)
dir <- "~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/csv/"
corrType = "spearman"
files <- list.files(dir)
summary = data.frame(
'pair' = character(),
'corr' = double(),
't-stat' = double(),
'p-val' = double()
)
for(file in files) {
set.seed(9999)
dat <- read_csv(paste0(dir, file)) %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
testResults <- cor.test(dat[, 1], dat[, 2], method = corrType)
dat <- data.frame(
pair = gsub('.csv', '', file),
corr = testResults$estimate,
't-stat' = testResults$statistic,
'p-val' = testResults$p.value
)
summary <- summary %>%
bind_rows(dat)
}
sum_dat <- summary %>%
separate(pair, sep = "_vs_", into = c("v1", "v2")) %>%
select("v1", "v2", corr, p.val)
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$corr
var <- unique(c(var1,var2))
corr <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
corr[lower.tri(corr,diag = FALSE)] <- r # lower triangular matrix to be r
corr <- Matrix::forceSymmetric(corr, uplo="L")
corr <- as.matrix(corr)
corr <- as.data.frame(corr) # formatting
row.names(corr) <- var # row names
colnames(corr) <- var # column names
var1 <- sum_dat$v1
var2 <- sum_dat$v2
r <- sum_dat$p.val
var <- unique(c(var1,var2))
pval <- matrix(1, nrow=6, ncol=6, byrow = T) # a matrix with 1s
pval[lower.tri(pval, diag = FALSE)] <- r # lower triangular matrix to be r
pval <- Matrix::forceSymmetric(pval, uplo="L")
pval <- as.matrix(pval)
pval <- as.data.frame(pval) # formatting
row.names(pval) <- var # row names
colnames(pval) <- var # column names
pval
write_csv(summary, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/summaryResults_", corrType, ".csv"))
write_csv(corr, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/corrMatrix_", corrType, ".csv"))
write_csv(pval, paste0("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/processed/pvalMatrix_", corrType, ".csv"))
warnings()
testResults <- cor.test(dat[, 1], dat[, 2], method = corrType)
summary
dat <- read_csv("~/Dropbox/manuscripts/RestorationMetaAnalysis/jacob/data/csv/fa_vs_fr.csv")
dat
dat  %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
newDat <- dat %>%
select(-"system:index", -".geo") %>%
as.data.frame() %>%
sample_n(1000)   # Randomly sample 1000 pts
newDat
testResults <- cor.test(newDat[, 1], newDat[, 2], method = corrType)
testResults
summary(testResults)
testResults$statistic
library(rgdal)
library(sf)
library(sp)
library(tidyverse)
library(rgdal)
library(sf)
library(sp)
library(tidyverse)
lulc2000 <- read_sf("~/Documents/dmcr_data/Land use (2000 and 2014)/MG_TYPE_43.shp") %>%
dplyr::select(code = CODE) %>%
mutate(code = ifelse(code %in% c("Mi"), "Unk", code))
lulc2014 <- read_sf("~/Documents/dmcr_data/Land use (2000 and 2014)/MG_TYPE_57_bffr.shp") %>%
dplyr::select(code = CODE) %>%
mutate(code = ifelse(code %in% c("S", "W"), "Unk", code))
lulc2000
lulc2014
?st_intersection()
lulcc <- st_intersection(lulc2000, lulc2014)
?st-write
?st_write
lulc2000 <- raster("~/Documents/dmcr_data/Land use (2000 and 2014)/mg2000.tif")
library(raster)
library(rgdal)
library(sf)
library(sp)
library(tidyverse)
lulc2000 <- raster("~/Documents/dmcr_data/Land use (2000 and 2014)/mg2000.tif")
lulc2014 <- raster("~/Documents/dmcr_data/Land use (2000 and 2014)/mg2014.tif")
lulc2000
table(lulc2000, lulc2014)
both <- stack(lulc2000, lulc2014)
both
values(both)
install.packages("greenbrown")
lulc2000
values(drop_na(lulc2000))
is.na(lulc2000)
lulc2000[0:100,]
extract(lulc2000, 1:100)
raster::extract(lulc2000, 1:100)
lulc2000
lulc2000_crop <- crop(lulc2000, e)
e <- extent(99.5, 101.5, 12.8, 13.8)
lulc2000_crop <- crop(lulc2000, e)
lulc2014_crop <- crop(lulc2014, e)
lulc2000_crop
e <- extent(100.5, 101.0, 12.8, 13.8)
lulc2000_crop <- crop(lulc2000, e)
lulc2014_crop <- crop(lulc2014, e)
lulc2000
lulc2000_crop
e <- extent(100.5, 101.0, 13.3, 13.8)
lulc2000_crop <- crop(lulc2000, e)
lulc2014_crop <- crop(lulc2014, e)
lulc2000_crop
e <- extent(100.8, 101.0, 13.6, 13.8)
lulc2000_crop <- crop(lulc2000, e)
lulc2014_crop <- crop(lulc2014, e)
lulc2000_crop
writeRaster
?writeRaster
